{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Information:\n",
    "\n",
    "### Team Member 1:\n",
    "* UNI:  \n",
    "* Name:\n",
    "\n",
    "### Team Member 2 [optional]:\n",
    "* UNI:  \n",
    "* Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0 - Import Libraries, Load Data [0 points]\n",
    "\n",
    "This is the basic step where you can load the data and create train and test sets for internal validation as per your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, scale, OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.feature_selection import RFE, SelectKBest, mutual_info_classif, f_classif\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "data = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate the ID column from the holdout set\n",
    "\n",
    "\n",
    "holdout = pd.read_csv('data/holdout.csv')\n",
    "ID = holdout['ID']\n",
    "holdout = holdout.drop('ID', 1)\n",
    "\n",
    "\n",
    "#Remove the \"duration\" feature to avoid getting overoptimistic results\n",
    "\n",
    "\n",
    "data = data.drop('duration', 1)\n",
    "holdout = holdout.drop('duration', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the \"month\" and \"day_of_week\" features to integers\n",
    "#(in order to experiment with treating them as both categorical and continuous features)\n",
    "\n",
    "\n",
    "months = [('mar', 3), ('apr', 4), ('may', 5), ('jun', 6), ('jul', 7), ('aug', 8),\n",
    "          ('sep', 9), ('oct', 10), ('nov', 11), ('dec', 12)]\n",
    "weekdays = [('mon', 1), ('tue', 2), ('wed', 3), ('thu', 4), ('fri', 5)]\n",
    "\n",
    "for month in months:\n",
    "    data['month'].replace(to_replace=month[0], value=month[1], inplace=True)\n",
    "    holdout['month'].replace(to_replace=month[0], value=month[1], inplace=True)\n",
    "    \n",
    "for day in weekdays:\n",
    "    data['day_of_week'].replace(to_replace=day[0], value=day[1], inplace=True)\n",
    "    holdout['day_of_week'].replace(to_replace=day[0], value=day[1], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a copy of the dataset for encoding purposes and leave the original for visualizations\n",
    "#Separate the categorical features from the numerical ones\n",
    "#Encode category and response variable strings to integers\n",
    "#Treat \"prev_days\" as a categorical variable due to the large majority of 999 values\n",
    "\n",
    "\n",
    "num = ['age', 'month', 'day_of_week', 'campaign', 'prev_days', 'prev_contacts','emp_var_rate',\n",
    "       'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed']\n",
    "\n",
    "cat = ['job', 'marital_status', 'education', 'credit_default', 'housing',\n",
    "       'loan', 'contact', 'prev_outcomes']\n",
    "\n",
    "\n",
    "enc_data = data.copy()\n",
    "enc_holdout = holdout.copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "for col in cat:\n",
    "    levels = np.concatenate((enc_data[col].unique(), enc_holdout[col].unique()))\n",
    "    le.fit(levels)\n",
    "    enc_data[col] = le.transform(enc_data[col])\n",
    "    enc_holdout[col] = le.transform(enc_holdout[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode the response variable\n",
    "#Separate the features from the response variable\n",
    "\n",
    "\n",
    "levels = enc_data['subscribed'].unique()\n",
    "le.fit(levels)\n",
    "enc_data['subscribed'] = le.transform(enc_data['subscribed'])\n",
    "\n",
    "X = enc_data.iloc[:,:-1]\n",
    "y = enc_data.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Balance the dataset using Random Undersampling, SMOTE, and SMOTE + Edited Nearest Neighbors\n",
    "#Create a different train/test split for each balanced dataset\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(replacement=True)\n",
    "sm = SMOTE()\n",
    "enn = SMOTEENN()\n",
    "\n",
    "X_rus, y_rus = rus.fit_sample(X, y)\n",
    "X_sm, y_sm = sm.fit_sample(X, y)\n",
    "X_enn, y_enn = enn.fit_sample(X, y)\n",
    "\n",
    "X_train_rus, X_test_rus, y_train_rus, y_test_rus = train_test_split(X_rus, y_rus, stratify=y_rus)\n",
    "X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm, stratify=y_sm)\n",
    "X_train_enn, X_test_enn, y_train_enn, y_test_enn = train_test_split(X_enn, y_enn, stratify=y_enn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 - Exploration and Preparation [10 points]\n",
    "\n",
    "In this step, we expect you to look into the data and try to understand it before modeling. This understanding may lead to some basic data preparation steps which are common across the two model sets required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get summary statistics on the data\n",
    "\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine the distribution of job categories\n",
    "\n",
    "\n",
    "job_count = Counter(data['job'])\n",
    "print(job_count)\n",
    "plt.bar(range(len(job_count)), job_count.values(), align='center')\n",
    "plt.xticks(range(len(job_count)), list(job_count.keys()), rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine the distribution of marital statuses\n",
    "\n",
    "\n",
    "marital_count = Counter(data['marital_status'])\n",
    "print(marital_count)\n",
    "plt.bar(range(len(marital_count)), marital_count.values(), align='center')\n",
    "plt.xticks(range(len(marital_count)), list(marital_count.keys()), rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine the distribution of education levels\n",
    "\n",
    "\n",
    "education_count = Counter(data['education'])\n",
    "print(education_count)\n",
    "plt.bar(range(len(education_count)), education_count.values(), align='center')\n",
    "plt.xticks(range(len(education_count)), list(education_count.keys()), rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine the distribution of call months\n",
    "\n",
    "\n",
    "month_count = Counter(enc_data['month'])\n",
    "print(month_count)\n",
    "plt.bar(range(len(month_count)), month_count.values(), align='center')\n",
    "plt.xticks(range(len(month_count)), list(month_count.keys()), rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine the distribution of days of the week\n",
    "\n",
    "\n",
    "week_count = Counter(enc_data['day_of_week'])\n",
    "print(week_count)\n",
    "plt.bar(range(len(week_count)), week_count.values(), align='center')\n",
    "plt.xticks(range(len(week_count)), list(week_count.keys()), rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine the distribution of previous days since last contact\n",
    "\n",
    "\n",
    "days_count = Counter(data['prev_days'])\n",
    "print(days_count)\n",
    "plt.bar(range(len(days_count)), days_count.values(), align='center')\n",
    "plt.xticks(range(len(days_count)), list(days_count.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examine distribution of binary variables\n",
    "\n",
    "\n",
    "default_count = Counter(data['credit_default'])\n",
    "housing_count = Counter(data['housing'])\n",
    "loan_count = Counter(data['loan'])\n",
    "contact_count = Counter(data['contact'])\n",
    "print(default_count)\n",
    "print(housing_count)\n",
    "print(loan_count)\n",
    "print(contact_count)\n",
    "\n",
    "ax1 = plt.subplot('331')\n",
    "plt.bar(range(len(default_count)), default_count.values(), align='center')\n",
    "plt.xticks(range(len(default_count)), list(default_count.keys()), rotation='vertical')\n",
    "ax1.set_title('Has Credit Defaults?')\n",
    "\n",
    "ax2 = plt.subplot('333')\n",
    "plt.bar(range(len(housing_count)), housing_count.values(), align='center')\n",
    "plt.xticks(range(len(housing_count)), list(housing_count.keys()), rotation='vertical')\n",
    "ax2.set_title('Has Housing Loans?')\n",
    "\n",
    "ax3 = plt.subplot('337')\n",
    "plt.bar(range(len(loan_count)), loan_count.values(), align='center')\n",
    "plt.xticks(range(len(loan_count)), list(loan_count.keys()), rotation='vertical')\n",
    "ax3.set_title('Has Personal Loans?')\n",
    "\n",
    "ax4 = plt.subplot('339')\n",
    "plt.bar(range(len(contact_count)), contact_count.values(), align='center')\n",
    "plt.xticks(range(len(contact_count)), list(contact_count), rotation='vertical')\n",
    "ax4.set_title('Method of Communication')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create covariance matrix heatmap to check for correlated features\n",
    "\n",
    "\n",
    "cov = np.cov(scale(X), rowvar = False)\n",
    "plt.imshow(cov, cmap='viridis', interpolation='nearest')\n",
    "plt.xticks(range(len(list(X))), list(X), rotation = 'vertical')\n",
    "plt.yticks(range(len(list(X))), list(X))\n",
    "\n",
    "\n",
    "#Take a closer look at the three continuous variables with high covariance\n",
    "\n",
    "\n",
    "print('Correlation Matrix')\n",
    "print('(nr_employed, euribor3m, emp_var_rate)')\n",
    "\n",
    "np.corrcoef(np.stack((data['nr_employed'], data['euribor3m']), axis=0), data['emp_var_rate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 - ModelSet1 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set1:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. Any classification algorithm covered in class apart from tree-based models can be tested here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine feature importance with RFE using SVMs\n",
    "\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc_rfe = RFE(svc, n_features_to_select = 10)\n",
    "\n",
    "sm = svc_rfe.fit(X_train_sm, y_train_sm)\n",
    "print('sm :', sm.ranking_)\n",
    "\n",
    "rus = svc_rfe.fit(X_train_rus, y_train_rus)\n",
    "print('rus:', rus.ranking_)\n",
    "\n",
    "enn = svc_rfe.fit(X_train_enn, y_train_enn)\n",
    "print('enn:', enn.ranking_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine feature importance with RFE using Logistic Regression\n",
    "\n",
    "\n",
    "lr = LogisticRegression(n_jobs=8)\n",
    "lr_rfe = RFE(lr, n_features_to_select = 10)\n",
    "\n",
    "sm_lr = lr_rfe.fit(X_train_sm, y_train_sm)\n",
    "rus_lr = lr_rfe.fit(X_train_rus, y_train_rus)\n",
    "enn_lr = lr_rfe.fit(X_train_enn, y_train_enn)\n",
    "\n",
    "print('sm :', sm_lr.ranking_)\n",
    "print('rus:', rus_lr.ranking_)\n",
    "print('enn:', enn_lr.ranking_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine feature importance with F-scores, mutual information, and SelectKBest\n",
    "\n",
    "\n",
    "f = SelectKBest(f_classif, k=10)\n",
    "mi = SelectKBest(mutual_info_classif, k=10)\n",
    "                \n",
    "f.fit(X_train_sm, y_train_sm)\n",
    "mi.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "print('f :', f.get_support())\n",
    "print('mi:', mi.get_support())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine feature importance with stability selection using Logistic Regression\n",
    "\n",
    "\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "\n",
    "rlr = RandomizedLogisticRegression()\n",
    "\n",
    "rus_rlr = rlr.fit(X_train_rus, y_train_rus)\n",
    "sm_rlr = rlr.fit(X_train_sm, y_train_sm)\n",
    "enn_rlr = rlr.fit(X_train_enn, y_train_enn)\n",
    "\n",
    "print('sm :', sm_rlr.scores_)\n",
    "print('rus:', rus_rlr.scores_)\n",
    "print('enn:', enn_rlr.scores_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the features deemed to be unimportant: \"age\", \"housing\", \"nr_employed\"\n",
    "#Perform One Hot Encoding on the categorical features\n",
    "#Determine the best resampling technique to use with Logistic Regression\n",
    "\n",
    "\n",
    "dropped = [0, 5, 18]\n",
    "leftover_discrete = np.array([0, 1, 2, 3, 4, 5, 11])\n",
    "levels = np.array([12, 4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "pipe = make_pipeline(MaxAbsScaler(), LogisticRegressionCV())\n",
    "\n",
    "rus_score = cross_val_score(pipe, X_train_rus_enc, y_train_rus, cv=10, scoring='roc_auc')\n",
    "print('rus:', np.mean(rus_score))\n",
    "\n",
    "sm_score = cross_val_score(pipe, X_train_sm_enc, y_train_sm, cv=10, scoring='roc_auc')\n",
    "print('sm :', np.mean(sm_score))\n",
    "\n",
    "enn_score = cross_val_score(pipe, X_train_enn_enc, y_train_enn, cv=10, scoring='roc_auc')\n",
    "print('enn:', np.mean(enn_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the features deemed to be unimportant: \"age\", \"job\", \"housing\", \"nr_employed\"\n",
    "#Perform One Hot Encoding on the categorical features\n",
    "#Determine the best resampling technique to use with Logistic Regression\n",
    "\n",
    "\n",
    "dropped = [0, 1, 5, 18]\n",
    "leftover_discrete = [0, 1, 2, 3, 4, 10]\n",
    "levels = np.array([4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "pipe = make_pipeline(MaxAbsScaler(), LogisticRegressionCV())\n",
    "\n",
    "rus_score = cross_val_score(pipe, X_train_rus_enc, y_train_rus, cv=10, scoring='roc_auc')\n",
    "print('rus:', np.mean(rus_score))\n",
    "\n",
    "sm_score = cross_val_score(pipe, X_train_sm_enc, y_train_sm, cv=10, scoring='roc_auc')\n",
    "print('sm :', np.mean(sm_score))\n",
    "\n",
    "enn_score = cross_val_score(pipe, X_train_enn_enc, y_train_enn, cv=10, scoring='roc_auc')\n",
    "print('enn:', np.mean(enn_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the features deemed to be unimportant: \"age\", \"job\", \"housing\", \"cons_price_idx\", \"nr_employed\"\n",
    "#Perform One Hot Encoding on the categorical features\n",
    "#Determine the best resampling technique to use with Logistic Regression\n",
    "\n",
    "\n",
    "dropped = [0, 1, 5, 15, 18]\n",
    "leftover_discrete = [0, 1, 2, 3, 4, 10]\n",
    "levels = np.array([4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "pipe = make_pipeline(MaxAbsScaler(), LogisticRegressionCV())\n",
    "\n",
    "rus_score = cross_val_score(pipe, X_train_rus_enc, y_train_rus, cv=10, scoring='roc_auc')\n",
    "print('rus:', np.mean(rus_score))\n",
    "\n",
    "sm_score = cross_val_score(pipe, X_train_sm_enc, y_train_sm, cv=10, scoring='roc_auc')\n",
    "print('sm :', np.mean(sm_score))\n",
    "\n",
    "enn_score = cross_val_score(pipe, X_train_enn_enc, y_train_enn, cv=10, scoring='roc_auc')\n",
    "print('enn:', np.mean(enn_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the features deemed to be unimportant: \"age\", \"job\", \"housing\", \"prev_days\", \"cons_price_idx\", \"nr_employed\"\n",
    "#Perform One Hot Encoding on the categorical features\n",
    "#Determine the best resampling technique to use with Logistic Regression\n",
    "\n",
    "\n",
    "dropped = [0, 1, 5, 11, 15, 18]\n",
    "leftover_discrete = [0, 1, 2, 3, 4, 9]\n",
    "levels = np.array([4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "pipe = make_pipeline(MaxAbsScaler(), LogisticRegressionCV())\n",
    "\n",
    "rus_score = cross_val_score(pipe, X_train_rus_enc, y_train_rus, cv=10, scoring='roc_auc')\n",
    "print('rus:', np.mean(rus_score))\n",
    "\n",
    "sm_score = cross_val_score(pipe, X_train_sm_enc, y_train_sm, cv=10, scoring='roc_auc')\n",
    "print('sm :', np.mean(sm_score))\n",
    "\n",
    "enn_score = cross_val_score(pipe, X_train_enn_enc, y_train_enn, cv=10, scoring='roc_auc')\n",
    "print('enn:', np.mean(enn_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Score the Logistic Regression model that produced the best results on the training data\n",
    "#Drop the \"age\", \"job\", \"housing\", \"cons_price_idx\", and \"nr_employed\" features\n",
    "\n",
    "\n",
    "dropped = [0, 1, 5, 15, 18]\n",
    "leftover_discrete = [0, 1, 2, 3, 4, 10]\n",
    "levels = np.array([4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "X_test_rus_enc = one.fit_transform(np.delete(X_test_rus, dropped, axis=1))\n",
    "X_test_sm_enc = one.fit_transform(np.delete(X_test_sm, dropped, axis=1))\n",
    "X_test_enn_enc = one.fit_transform(np.delete(X_test_enn, dropped, axis=1))\n",
    "\n",
    "pipe = make_pipeline(MaxAbsScaler(), LogisticRegressionCV(cv=10, scoring='roc_auc'))\n",
    "\n",
    "rus_lr = pipe.fit(X_train_rus_enc, y_train_rus)\n",
    "print('rus:', rus_lr.score(X_test_rus_enc, y_test_rus))\n",
    "\n",
    "sm_lr = pipe.fit(X_train_sm_enc, y_train_sm)\n",
    "print('sm :', sm_lr.score(X_test_sm_enc, y_test_sm))\n",
    "\n",
    "enn_lr = pipe.fit(X_train_enn_enc, y_train_enn)\n",
    "print('enn:', enn_lr.score(X_test_enn_enc, y_test_enn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine the best resampling technique to use with SGDClassifier and the chosen feature set\n",
    "\n",
    "\n",
    "dropped = [0, 1, 5, 15, 18]\n",
    "leftover_discrete = [0, 1, 2, 3, 4, 10]\n",
    "levels = np.array([4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "grid = {'alpha': np.logspace(-1,7, 9)}\n",
    "\n",
    "methods = [('rus', X_train_rus, y_train_rus), ('sm ', X_train_sm, y_train_sm), ('enn', X_train_enn, y_train_enn)]\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "for array in methods:\n",
    "    gridsearch = GridSearchCV(SGDClassifier(loss='modified_huber', n_iter=np.ceil(10**6/np.shape(array[1])[0])),\n",
    "                                     param_grid=grid, scoring='roc_auc', cv=15)\n",
    "    \n",
    "    sgd = gridsearch.fit(scaler.fit_transform(array[1]), array[2])\n",
    "    print(array[0], 'best score:', sgd.score(array[1], array[2]))\n",
    "    print(array[0], 'best parameters:', sgd.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Score the SGDClassifier model with the best parameter value and the chosen feature set\n",
    "\n",
    "\n",
    "dropped = [0, 1, 5, 15, 18]\n",
    "leftover_discrete = [0, 1, 2, 3, 4, 10]\n",
    "levels = np.array([4, 8, 3, 3, 2, 3])\n",
    "\n",
    "one = OneHotEncoder(categorical_features=leftover_discrete, n_values=levels)\n",
    "\n",
    "X_train_rus_enc = one.fit_transform(np.delete(X_train_rus, dropped, axis=1))\n",
    "X_train_sm_enc = one.fit_transform(np.delete(X_train_sm, dropped, axis=1))\n",
    "X_train_enn_enc = one.fit_transform(np.delete(X_train_enn, dropped, axis=1))\n",
    "\n",
    "X_test_rus_enc = one.fit_transform(np.delete(X_test_rus, dropped, axis=1))\n",
    "X_test_sm_enc = one.fit_transform(np.delete(X_test_sm, dropped, axis=1))\n",
    "X_test_enn_enc = one.fit_transform(np.delete(X_test_enn, dropped, axis=1))\n",
    "\n",
    "sgd_pipe = make_pipeline(MaxAbsScaler(),\n",
    "                         SGDClassifier(alpha=0.1,loss='modified_huber',n_iter=np.ceil(10**6/np.shape(X_train_enn_enc)[0])))\n",
    "\n",
    "rus_sgd = sgd_pipe.fit(X_train_rus_enc, y_train_rus)\n",
    "print('rus:', roc_auc_score(rus_sgd.predict(X_test_rus_enc), y_test_rus))\n",
    "\n",
    "sm_sgd = sgd_pipe.fit(X_train_sm_enc, y_train_sm)\n",
    "print('sm :', roc_auc_score(sm_sgd.predict(X_test_sm_enc), y_test_sm))\n",
    "\n",
    "enn_sgd = sgd_pipe.fit(X_train_enn_enc, y_train_enn)\n",
    "print('enn:', roc_auc_score(enn_sgd.predict(X_test_enn_enc), y_test_enn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 - ModelSet2 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set2:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. We encourage you to try decition tree, random forest and gradient boosted tree methods here and pick the one which you think works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform a grid search to find the optimal Decision Tree parameter values for max_depth and max_features\n",
    "\n",
    "\n",
    "grid = {'max_features': range(3, 15, 1), 'max_depth': np.linspace(5, 50, 2)}\n",
    "\n",
    "search = GridSearchCV(DecisionTreeClassifier(), param_grid = grid, scoring='roc_auc', cv=10)\n",
    "\n",
    "rus = search.fit(X_train_rus, y_train_rus)\n",
    "sm = search.fit(X_train_sm, y_train_sm)\n",
    "enn = search.fit(X_train_enn, y_train_enn)\n",
    "\n",
    "print('rus:', rus.score(X_train_rus, y_train_rus))\n",
    "print(rus.best_params_)\n",
    "print('sm :', sm.score(X_train_sm, y_train_sm))\n",
    "print(sm.best_params_)\n",
    "print('enn:', enn.score(X_train_enn, y_train_enn))\n",
    "print(enn.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform cross-validation using each balanced dataset and the optimal parameters\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(max_features=11, max_depth=50)\n",
    "\n",
    "print('rus:', np.mean(cross_val_score(dt, X_train_rus, y_train_rus, cv=10, scoring='roc_auc')))\n",
    "print('sm :', np.mean(cross_val_score(dt, X_train_sm, y_train_sm, cv=10, scoring='roc_auc')))\n",
    "print('enn:', np.mean(cross_val_score(dt, X_train_enn, y_train_enn, cv=10, scoring='roc_auc')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Score the test set using the chosen parameters\n",
    "\n",
    "\n",
    "rus_dt = dt.fit(X_train_rus, y_train_rus)\n",
    "sm_dt = dt.fit(X_train_sm, y_train_sm)\n",
    "enn_dt = dt.fit(X_train_enn, y_train_enn)\n",
    "\n",
    "print('rus:', roc_auc_score(rus_dt.predict(X_test_rus), y_test_rus))\n",
    "print('sm:', roc_auc_score(sm_dt.predict(X_test_sm), y_test_sm))\n",
    "print('enn:', roc_auc_score(enn_dt.predict(X_test_enn), y_test_enn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "grid = {'max_depth': np.linspace(5, 50, 2)}\n",
    "search = GridSearchCV(RandomForestClassifier(n_estimators=100, max_features='sqrt'), param_grid = grid, scoring='roc_auc', cv=10)\n",
    "#rt = RandomForestClassifier(n_estimators=200, max_features='sqrt', max_depth=10)\n",
    "\n",
    "rus_rt = search.fit(X_train_rus, y_train_rus)\n",
    "sm_rt = search.fit(X_train_sm, y_train_sm)\n",
    "enn_rt = search.fit(X_train_enn, y_train_enn)\n",
    "\n",
    "print('rus:', rus_rt.score(X_train_rus, y_train_rus))\n",
    "print(rus_rt.best_params_)\n",
    "print('sm :', sm_rt.score(X_train_sm, y_train_sm))\n",
    "print(sm_rt.best_params_)\n",
    "print('enn:', enn_rt.score(X_train_enn, y_train_enn))\n",
    "print(enn_rt.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('rus:', roc_auc_score(rus_rt.predict(X_test_rus), y_test_rus))\n",
    "print('sm:', roc_auc_score(sm_rt.predict(X_test_sm), y_test_sm))\n",
    "print('enn:', roc_auc_score(enn_rt.predict(X_test_enn), y_test_enn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 - Ensemble [20 points + 10 Bonus points]\n",
    "\n",
    "In this step, we expect you to use the models created before and create new predictions. You should definitely try poor man's stacking but we encourage you to think of different ensemble techniques as well. We will judge your creativity and improvement in model performance using ensemble models and you can potentially earn 10 bonus points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write code below, you can make multiple cells\n",
    "assert True"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
